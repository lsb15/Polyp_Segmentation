{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/4uiiurz1/pytorch-nested-unet"
      ],
      "metadata": {
        "id": "uEKQwYqJdIXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Define paths to data directories\n",
        "DATA_DIR = './inputs/polyp/'\n",
        "image_dir = os.path.join(DATA_DIR, 'images')\n",
        "mask_dir = os.path.join(DATA_DIR, 'masks')"
      ],
      "metadata": {
        "id": "KyPSWHm_dFpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read an example image and its corresponding mask\n",
        "img = cv2.imread(os.path.join(image_dir, os.listdir(image_dir)[0]))\n",
        "mask = cv2.imread(os.path.join(mask_dir, os.listdir(mask_dir)[0]))\n",
        "\n",
        "# Visualize the example image and mask\n",
        "plt.subplot(1,2,1)\n",
        "plt.title('IMAGE')\n",
        "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.title('MASK')\n",
        "plt.imshow(cv2.cvtColor(mask, cv2.COLOR_BGR2RGB))"
      ],
      "metadata": {
        "id": "KWDcPSsMc_p4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OAcdmRRLdOnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model using train.py script\n",
        "!python train.py --dataset polyp --arch NestedUNet --name polyp_segmentation --epochs 150 --batch_size 8 --input_w 384 --input_h 384 --img_ext jpg --mask_ext jpg --optimizer Adam"
      ],
      "metadata": {
        "id": "fI-DV4q5dSfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bdvGd6cHdTg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validating the trained model using val.py script\n",
        "!python val.py --name polyp_segmentation"
      ],
      "metadata": {
        "id": "qkIB9MfBdA_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EFEWPE-VdUxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import archs\n",
        "import yaml\n",
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "rxuDBimpU1dO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained model weights\n",
        "best_model = 'path to weight file'\n",
        "# Load the configuration file for the trained model\n",
        "yml_path = 'models/polyp_segmentation/config.yml'"
      ],
      "metadata": {
        "id": "Vf8orApkU1fk"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(yml_path) as f:\n",
        "    data = yaml.load(f, Loader=yaml.FullLoader)\n",
        "print(data)"
      ],
      "metadata": {
        "id": "CsaEMjJ8dD_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the device for model inference (GPU if available, otherwise CPU)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model = archs2.__dict__[data['arch']](data['num_classes'],data['input_channels'],data['deep_supervision'])\n",
        "model = model.to(DEVICE)\n",
        "model.load_state_dict(torch.load(best_model, map_location=DEVICE))\n",
        "print(\"model loaded\")"
      ],
      "metadata": {
        "id": "FLiTHaH5U1l7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform inference on test images and save segmentation results\n",
        "test_folder = 'path to test folder'\n",
        "gt_folder = 'path to groundtruth mask of test folder'\n",
        "result_folder = 'path to result folder'\n",
        "\n",
        "if not os.path.exists(result_folder):\n",
        "    os.makedirs(result_folder)\n",
        "\n",
        "test_file_list = os.listdir(test_folder)"
      ],
      "metadata": {
        "id": "JY7qo34ieCzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to blend original image with predicted mask\n",
        "def blend_images(ori, pred):\n",
        "    ori = cv2.cvtColor(ori, cv2.COLOR_BGR2RGB)\n",
        "    output = Image.fromarray(pred)\n",
        "    background = Image.fromarray(ori).convert('RGBA')\n",
        "    output = output.resize((ori.shape[1], ori.shape[0])).convert('RGBA')\n",
        "    output_final = Image.blend(background, output, alpha=0.5)\n",
        "    return cv2.cvtColor(np.array(output_final), cv2.COLOR_BGR2RGB)"
      ],
      "metadata": {
        "id": "zT6DqHu7eEqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process each test image\n",
        "for idx, file in enumerate(test_file_list):\n",
        "    black = np.zeros(shape=(384,384*4,3), dtype=np.uint8)\n",
        "    # Load ground truth mask and resize it\n",
        "    gt = cv2.imread(os.path.join(gt_folder, file))\n",
        "    gt = cv2.resize(gt, (384,384))\n",
        "\n",
        "    # Load test image and resize it\n",
        "    img = cv2.imread(os.path.join(test_folder, file))\n",
        "    img = cv2.resize(img, (384,384))\n",
        "\n",
        "    # Preprocess input image for model inference\n",
        "    input = img.astype('float32') / 255\n",
        "    input = np.expand_dims(input, axis=0)\n",
        "    input = torch.from_numpy(input).to(DEVICE)\n",
        "    input = input.permute(0,3,1,2)\n",
        "\n",
        "    # Perform model inference\n",
        "    output = model(input)\n",
        "    output = torch.sigmoid(output)\n",
        "    output = output.permute(0,2,3,1).cpu().detach()\n",
        "\n",
        "    # Post-process predicted mask\n",
        "    pred = np.array(output[0])*255\n",
        "    pred = np.where(pred<240, 0, pred)\n",
        "    pred_ = np.repeat(pred, 3, -1).astype(np.uint8)\n",
        "\n",
        "    # Blend original image with predicted mask\n",
        "    output_final = blend_images(img, pred_)[:,:,:3]\n",
        "\n",
        "    cv2.putText(img, \"Origninal Image\", (70,40),cv2.FONT_HERSHEY_DUPLEX, 1,(0,0,255), thickness=3, lineType=cv2.LINE_AA)\n",
        "    cv2.putText(gt, \"GroundTruth Mask\", (60,40),cv2.FONT_HERSHEY_DUPLEX, 1,(0,0,255), thickness=3, lineType=cv2.LINE_AA)\n",
        "    cv2.putText(pred_, \"Predicted Mask\", (70,40),cv2.FONT_HERSHEY_DUPLEX, 1,(0,0,255), thickness=3, lineType=cv2.LINE_AA)\n",
        "    cv2.putText(output_final, \"Blended Images\", (60,40),cv2.FONT_HERSHEY_DUPLEX, 1,(0,0,255), thickness=3, lineType=cv2.LINE_AA)\n",
        "\n",
        "    # Create a visualization grid\n",
        "    black[:,:384,:] = img[:,:,:]\n",
        "    black[:,384:384*2,:] = gt[:,:,:]\n",
        "    black[:,384*2:384*3,:] = pred_[:,:,:]\n",
        "    black[:,384*3:384*4,:] = output_final[:,:,::]\n",
        "\n",
        "    # Save the visualization\n",
        "    cv2.imwrite(os.path.join(result_folder, file), black)\n",
        "\n",
        "    # Display the visualization (for the first 10 images)\n",
        "    if idx <10:\n",
        "        plt.imshow(cv2.cvtColor(black, cv2.COLOR_BGR2RGB))\n",
        "        plt.show()\n",
        "\n",
        "    if idx == 99:\n",
        "        break\n"
      ],
      "metadata": {
        "id": "4Ez9GCFaeEsn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}